#! /usr/bin/env python

import fileinput
import json
import nltk
import sys

# For each tuple..
for line in fileinput.input():
  '''
  From: SELECT * FROM document
  To: mention(id, doc_id, text_contents, type)
  '''

  row = json.loads(line)

  docid = row["document.id"]
  raw_text = row["document.raw_text"]

  if docid is not None and raw_text is not None:
    # perform named-entity recognition (NER) on the raw text (use nltk)

    # POS tagging on sentences
    sentences = nltk.sent_tokenize(raw_text)
    sentences = [nltk.word_tokenize(sent) for sent in sentences]
    sentences = [nltk.pos_tag(sent) for sent in sentences]

    # NER on each sentence
    for sent in sentences:
        chunked_sent = nltk.ne_chunk(sent)

        for child in chunked_sent.subtrees():
            # get only the mentions for PERSON entities
            if child.node == "PERSON":
                node_value = " ".join([x[0] for x in child])

                # get everything except for the first and last characters since encoding as utf8
                # and then getting the string representation will wrap the text in quotes
                mention_text = node_value.encode("utf-8").__repr__()[1:-1]

                # must specify every column except for id, which will be automatically generated by psql
                print json.dumps({
                  "doc_id": docid,
                  "text_contents": mention_text,
                  "type": "PERSON"
                })